{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_regression_project_explained.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q2RZWjxLZPX",
        "colab_type": "text"
      },
      "source": [
        "# Built a linear regression algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4U4xsR1Mp3n",
        "colab_type": "text"
      },
      "source": [
        "**Terminology:** <br>\n",
        " Let there be $n$ number of features and $m$ number of observations. Denote $x_n^{(i)}$ as the $i^{th}$ observation of the $n^{th}$ feature and $x_n$ as the $n^{th}$ feature. Define some hypothesis function: <br> <center>$$h_θ(x)=\\sum_{i=0}^{k} θ_i * f_i(x)$$</center><br>\n",
        " for some $k$, some $θ_j$ and $f_j$, where $θ = (θ_0,...,θ_j)$ and $f_0(x) = 1$. <br>\n",
        " The functions $f$ will be definied. Our job is to find the best values of $θ_j$ to minimize the squares.<br>\n",
        " Now set the cost function as:\n",
        " <center>$$J(θ) = \\frac{1}{m}\\sum_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2$$ </center>\n",
        "\n",
        " **Example:** <br>\n",
        " Let $n=1$ and $k=2$. Define $f_1(x) := x$ and $f_2(x) := x^2$. Then: <br>\n",
        "<center>$$h_θ(x) = θ_0  + \\theta_1 * x + \\theta_2 * x^2$$</center>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ35Pju5gJiy",
        "colab_type": "text"
      },
      "source": [
        "# Cost Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up7XVtL6gOX8",
        "colab_type": "text"
      },
      "source": [
        "Let us start by defining some values that the users will give us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBpwx1BzMdTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [[1,2,3,4,5],[1,1,1,1,2]]\n",
        "result = [4,6,8,10,13]\n",
        "alpha = 0.001\n",
        "variable_names = ['x','y']\n",
        "hypothesis_equation = ['pow(x,2)','x','y','1'] # the result should be y+2x+1\n",
        "initial_guess = [10,5,3,4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzOFx1C-B9hz",
        "colab_type": "text"
      },
      "source": [
        "Each element of the hypothesis equation list is an equation $f_j$. Now we will turn those equations into linear because the algorithm will run faster. The plan is to apply the dataset into every $f_j$ and so create a new set of features $x_n^{'}$ for $n \\in [1,k] \\subsetℕ$. Then: <br>\n",
        "<center>$$h_\\theta^{'}(x^{'})=\\sum_{i=0}^{k} \\theta_i * x_i^{'}$$</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHoWKi2cB82_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_prime = [[] for i in range(len(hypothesis_equation))]\n",
        "for i in range(len(data[0])):\n",
        "  for j in range(len(variable_names)):\n",
        "    exec('%s = %.7f' % (variable_names[j],data[j][i]))\n",
        "  for n in range(len(hypothesis_equation)):\n",
        "    data_prime[n].append(eval(hypothesis_equation[n]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smYEGf4uHHNN",
        "colab_type": "code",
        "outputId": "ac49eb91-7e9c-45fd-c675-95e3f9c9382a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "for i in data_prime:\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 4.0, 9.0, 16.0, 25.0]\n",
            "[1.0, 2.0, 3.0, 4.0, 5.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 2.0]\n",
            "[1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd2VjkjKHSum",
        "colab_type": "text"
      },
      "source": [
        "So the code worked: <br>\n",
        "For example $[1,4,9,16,25]$ is each element in $x$ squared."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0c-E3RyJxNz",
        "colab_type": "text"
      },
      "source": [
        "We start by calculating the hypothesis equation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbaOLZ4aKHvq",
        "colab_type": "text"
      },
      "source": [
        "$$h_\\theta(x)=\\sum_{i=0}^{k} \\theta_i * x_i$$<br>\n",
        "$$=\\theta_0*x_0+\\theta_1*x_1 +....+\\theta_k *x_k$$ <br>\n",
        "$$=\\theta*x^{T}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIDkmdHGLP_e",
        "colab_type": "text"
      },
      "source": [
        "Now for the cost function,using the above function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwXZkphFLS4l",
        "colab_type": "text"
      },
      "source": [
        "$$J(\\theta) = \\frac{1}{m}\\sum_{i=0}^{m} (\\theta*{x^{(i)}}^T-y^{(i)})^2$$<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnmXpUnJM4mr",
        "colab_type": "text"
      },
      "source": [
        "Turn everything into np.array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAVVddINH2KE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0itIKyilyMF",
        "colab_type": "code",
        "outputId": "6fb71dc8-97b6-43a8-d6d5-eda134777ebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "list_of_obsrervations[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[1, 1, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXFdARzvM8no",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = np.matrix(data_prime)\n",
        "list_of_obsrervations = m.transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtQYtYhTNQTw",
        "colab_type": "code",
        "outputId": "9a63b27a-c08d-4e7e-979b-452d207fa686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "list_of_obsrervations"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[ 1,  1,  1,  1],\n",
              "        [ 4,  2,  1,  1],\n",
              "        [ 9,  3,  1,  1],\n",
              "        [16,  4,  1,  1],\n",
              "        [25,  5,  2,  1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JNHrAJRhQbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cost_function(thita_list):\n",
        "  total = 0 \n",
        "  for i in range(list_of_obsrervations.shape[0]):\n",
        "    total += pow(int(np.dot(list_of_obsrervations[i],np.array(thita_list))) - result[i],2)\n",
        "  return(total/(2*len(result)))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga82LXr56VC_",
        "colab_type": "code",
        "outputId": "9bec48ea-3714-4ff1-f508-cccc2cff7036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cost_function([1,1,1,2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTIMBww0kpS8",
        "colab_type": "text"
      },
      "source": [
        "# Partial Derivative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd64ESzdk1OY",
        "colab_type": "text"
      },
      "source": [
        "Now we would like to find a way to calculate partial derivatives. I tried using the definition: <br>\n",
        "<center>$$\\frac{\\partial}{\\partial x_k} f(x_0,...,x_k,...x_n)= \\lim_{\\epsilon \\to 0} \\frac{f(x_0,...,x_k+\\epsilon,...,x_n)-f(x)}{\\epsilon}$$</center>\n",
        "But it was slow. So we will calculate the derivatives ourselfs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d1uAMA1bCPT",
        "colab_type": "text"
      },
      "source": [
        "<center>$$J(\\theta) = \\frac{1}{m}\\sum_{i=0}^{m} (\\theta*{x^{(i)}}^T-y^{(i)})^2$$</center>\n",
        "So, <br>\n",
        "$$\\frac{\\partial}{\\partial\\theta_k}J(\\theta)=\\frac{\\partial}{\\partial\\theta_k} \\frac{1}{m}\\sum_{i=0}^{m} (\\theta*{x^{(i)}}^T-y^{(i)})^2$$\n",
        "<center>$$=\\frac{1}{m}\\sum_{i=0}^{m} \\frac{\\partial}{\\partial\\theta_k}(\\theta*{x^{(i)}}^T-y^{(i)})^2$$</center>\n",
        "Since $\\frac{d}{dx}(f_1+f_2)=\\frac{d}{dx}f_1+\\frac{d}{dx}f_2$. Now everything in that parenthsis will be a constant except the value of $\\theta_k$ that will have a coefficient of $x_k^{(i)}$. So,<br>\n",
        "$$=\\frac{1}{m}\\sum_{i=0}^{m} \\frac{\\partial}{\\partial\\theta_k}(x_1^{(i)}*\\theta_1+...+x^{(i)}_k*\\theta_k+...+x_n^{(i)}*\\theta_n - y^{(i)})^2$$\n",
        "$$=\\frac{1}{m}\\sum_{i=0}^{m}\\frac{\\partial}{\\partial\\theta_k}((x_k^{(i)}*\\theta_k)^2+(...)^2+2*(...)*(x_k^{(i)}*\\theta_k))$$\n",
        "$$=\\frac{1}{m}\\sum_{i=0}^{m}(2*{x_k^{(i)}}^2*\\theta_k+2*(...)*x_k^{(i)})$$\n",
        "$$=\\frac{2}{m}\\sum_{i=0}^{m}({x_k^{(i)}}*\\theta_k+(...))*x_k^{(i)}$$\n",
        "$$=\\frac{2}{m}\\sum_{i=0}^{m}((x_1^{(i)}*\\theta_1+...+x^{(i)}_k*\\theta_k+...+x_n^{(i)}*\\theta_n - y^{(i)}))*x_k^{(i)}$$\n",
        "$$=\\frac{2}{m}\\sum_{i=0}^{m}(\\theta*{x^{(i)}}^T-y^{(i)})*x_k^{(i)}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J636zxh5o6Lh",
        "colab_type": "text"
      },
      "source": [
        "So we have the equation for our case. Now is time to code it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tup6qn1yo-BS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def partial_derivative(thita_list,k):\n",
        "  total = 0 \n",
        "  for i in range(list_of_obsrervations.shape[0]):\n",
        "    total += (int(np.dot(list_of_obsrervations[i],np.array(thita_list))) - result[i])*list_of_obsrervations.item(i,k)\n",
        "  return(total/len(result))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSNoXP1UnDXK",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-kzj8E5dnRuO"
      },
      "source": [
        "**Gradient Descent:** <br>\n",
        "We will optimize the function using Gradient Descent. Keep in mind that we want to minimize $J(\\theta)$ and this function only takes values of $\\theta$ not $x$. Define some $\\alpha$ and start with some initial guess $\\theta^{(0)}=[\\theta_0,\\theta_1,...\\theta_j]$. Now let: <br>\n",
        "<center>$p^{(k-1)} =[\\frac{\\partial}{\\partial\\theta_0}J(\\theta^{(k-1)}),...,\\frac{\\partial}{\\partial\\theta_j}J(\\theta^{(k-1)})]$</center>\n",
        "Lastly,\n",
        "<center>$\\theta^{(k)} = \\theta^{(k-1)} - \\alpha * p^{(k-1)}$</center>  <br>\n",
        "\n",
        "And this continues as much as we like.<br>\n",
        "Basically, we calculate the partial derivative with respect to each $\\theta_k$ and evaluate at $\\theta$ and multiply by the learning rate. Then we substract that result from the previous value. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCsqsfNepw-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azeHoZ2Nmv3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(timeout_error = 15,alpha=0.001,initial_guess=[10,7,6,1]):                \n",
        "        start = time()\n",
        "        now = time()\n",
        "        current_thita_list = initial_guess\n",
        "        \n",
        "        while now - start < timeout_error:\n",
        "\n",
        "            partial_derivatives = [partial_derivative(current_thita_list,i) for i in range(len(current_thita_list))]\n",
        "\n",
        "            if [round(i,5) for i in partial_derivatives] == [float(0) for i in range(len(partial_derivatives))]:\n",
        "              return([round(j,3) for j in current_thita_list])\n",
        "              break\n",
        "\n",
        "            current_thita_list =[current_thita_list[i] - alpha*partial_derivatives[i] for i in range(len(partial_derivatives))]\n",
        "                \n",
        "            now = time()\n",
        "        \n",
        "        return([round(j,3) for j in current_thita_list])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFnxB_k2pcxL",
        "colab_type": "code",
        "outputId": "a353a368-ac0a-41b2-a24b-49895a504b8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "s = time()\n",
        "output=gradient_descent(initial_guess=[12,7,3,2])\n",
        "print(f'Result : {output} and finished in {round(time()-s,3)} seconds')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Result : [-0.209, 3.009, 1.361, 0.453] and finished in 0.322 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Csoa__Yhp_cC",
        "colab_type": "text"
      },
      "source": [
        "So it did not find the right answer because we have an x squared. But let us see how close it got."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Y1O0CDpf0B",
        "colab_type": "code",
        "outputId": "3d826bfb-753d-4c8e-ff74-977cd4d7eb3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cost_function(output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o5eyp9Tvb7a",
        "colab_type": "text"
      },
      "source": [
        "It actually found a second solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhkhNoo3v0Q5",
        "colab_type": "text"
      },
      "source": [
        "# Some extra features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i23mZYePv3gx",
        "colab_type": "text"
      },
      "source": [
        "So we would like to add some nice to have features before turning it into a class function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb0CSIVP9gmv",
        "colab_type": "text"
      },
      "source": [
        "## Live Plot of Cost Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vppw9Mii22il",
        "colab_type": "text"
      },
      "source": [
        "I have added a feature to live plot the cost function. <br>\n",
        "<br>\n",
        "![alt text](https://drive.google.com/uc?id=1ftJKgVXKCmTj86Jd5wosXpxum7TkbnIM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u35gBn5_sMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}